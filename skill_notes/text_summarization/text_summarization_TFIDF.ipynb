{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization with TDIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of text summarization\n",
    "- extractive summarization\n",
    "    - it consits of text taken from the original document\n",
    "- abstractive summarization\n",
    "    - it can contains novel sequences of text not necessarily taken from the input\n",
    "    - transformers, seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure\n",
    "\n",
    "1. Split the document into sentences (nltk.sent_tokenize(your_text))\n",
    "2. Compute the TF-IDF matrix from list of sentences\n",
    "2. Score each sentence by taking average non-zero TF-IDf values\n",
    "3. Sort(rank) each sentence by those scores\n",
    "4. Print the top scoring sentences as the summary\n",
    "\n",
    "- Note: extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import textwrap\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\github\\\\NLP-Projects\\\\skill_notes\\\\text_summarization'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('d:/github/NLP-Projects/skill_notes/data/bbc_full_text_cls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df[df.label == 'business']['text'].sample(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(x):\n",
    "    return textwrap.fill(x, replace_whitespace=False, fix_sentence_endings=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christmas sales worst since 1981\n",
      "\n",
      "UK retail sales fell in December,\n",
      "failing to meet expectations and making it by some counts the worst\n",
      "Christmas since 1981.\n",
      "\n",
      "Retail sales dropped by 1% on the month in\n",
      "December, after a 0.6% rise in November, the Office for National\n",
      "Statistics (ONS) said.  The ONS revised the annual 2004 rate of growth\n",
      "down from the 5.9% estimated in November to 3.2%. A number of\n",
      "retailers have already reported poor figures for December.  Clothing\n",
      "retailers and non-specialist stores were the worst hit with only\n",
      "internet retailers showing any significant growth, according to the\n",
      "ONS.\n",
      "\n",
      "The last time retailers endured a tougher Christmas was 23 years\n",
      "previously, when sales plunged 1.7%.\n",
      "\n",
      "The ONS echoed an earlier\n",
      "caution from Bank of England governor Mervyn King not to read too much\n",
      "into the poor December figures.  Some analysts put a positive gloss on\n",
      "the figures, pointing out that the non-seasonally-adjusted figures\n",
      "showed a performance comparable with 2003. The November-December jump\n",
      "last year was roughly comparable with recent averages, although some\n",
      "way below the serious booms seen in the 1990s.  And figures for retail\n",
      "volume outperformed measures of actual spending, an indication that\n",
      "consumers are looking for bargains, and retailers are cutting their\n",
      "prices.\n",
      "\n",
      "However, reports from some High Street retailers highlight\n",
      "the weakness of the sector.  Morrisons, Woolworths, House of Fraser,\n",
      "Marks & Spencer and Big Food all said that the festive period was\n",
      "disappointing.\n",
      "\n",
      "And a British Retail Consortium survey found that\n",
      "Christmas 2004 was the worst for 10 years.  Yet, other retailers -\n",
      "including HMV, Monsoon, Jessops, Body Shop and Tesco - reported that\n",
      "festive sales were well up on last year.  Investec chief economist\n",
      "Philip Shaw said he did not expect the poor retail figures to have any\n",
      "immediate effect on interest rates.  \"The retail sales figures are\n",
      "very weak, but as Bank of England governor Mervyn King indicated last\n",
      "night, you don't really get an accurate impression of Christmas\n",
      "trading until about Easter,\" said Mr Shaw.  \"Our view is the Bank of\n",
      "England will keep its powder dry and wait to see the big picture.\"\n"
     ]
    }
   ],
   "source": [
    "print(wrap(doc.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Christmas sales worst since 1981\\n\\nUK retail sales fell in December, failing to meet expectations and making it by some counts the worst Christmas since 1981.',\n",
       " 'Retail sales dropped by 1% on the month in December, after a 0.6% rise in November, the Office for National Statistics (ONS) said.',\n",
       " 'The ONS revised the annual 2004 rate of growth down from the 5.9% estimated in November to 3.2%.',\n",
       " 'A number of retailers have already reported poor figures for December.',\n",
       " 'Clothing retailers and non-specialist stores were the worst hit with only internet retailers showing any significant growth, according to the ONS.',\n",
       " 'The last time retailers endured a tougher Christmas was 23 years previously, when sales plunged 1.7%.',\n",
       " 'The ONS echoed an earlier caution from Bank of England governor Mervyn King not to read too much into the poor December figures.',\n",
       " 'Some analysts put a positive gloss on the figures, pointing out that the non-seasonally-adjusted figures showed a performance comparable with 2003.',\n",
       " 'The November-December jump last year was roughly comparable with recent averages, although some way below the serious booms seen in the 1990s.',\n",
       " 'And figures for retail volume outperformed measures of actual spending, an indication that consumers are looking for bargains, and retailers are cutting their prices.',\n",
       " 'However, reports from some High Street retailers highlight the weakness of the sector.',\n",
       " 'Morrisons, Woolworths, House of Fraser, Marks & Spencer and Big Food all said that the festive period was disappointing.',\n",
       " 'And a British Retail Consortium survey found that Christmas 2004 was the worst for 10 years.',\n",
       " 'Yet, other retailers - including HMV, Monsoon, Jessops, Body Shop and Tesco - reported that festive sales were well up on last year.',\n",
       " 'Investec chief economist Philip Shaw said he did not expect the poor retail figures to have any immediate effect on interest rates.',\n",
       " '\"The retail sales figures are very weak, but as Bank of England governor Mervyn King indicated last night, you don\\'t really get an accurate impression of Christmas trading until about Easter,\" said Mr Shaw.',\n",
       " '\"Our view is the Bank of England will keep its powder dry and wait to see the big picture.\"']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(doc.iloc[0].split(\"\\n\\n\", maxsplit=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tile is removed by selecting split(\"\\n\\n\", 1)\n",
    "sentences = nltk.sent_tokenize(doc.iloc[0].split(\"\\n\\n\",1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In L1 normalization, the relative importance of terms in a sentence is preserved, but the magnitude of the vector is scaled down so that it sums to 1.\n",
    "# This ensures that longer sentences (which naturally have more terms and larger raw TF-IDF values) don't dominate the scoring simply due to their length.\n",
    "featurizer = TfidfVectorizer(\n",
    "    stop_words=stopwords.words('english'),\n",
    "    norm='l1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = featurizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17x154 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 212 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t0.1808607792791759\n",
      "  (0, 30)\t0.11871441769336029\n",
      "  (0, 47)\t0.10999441338787577\n",
      "  (0, 89)\t0.1808607792791759\n",
      "  (0, 99)\t0.14165079067847455\n",
      "  (0, 110)\t0.15792440629406163\n",
      "  (0, 113)\t0.10999441338787577\n"
     ]
    }
   ],
   "source": [
    "print(X[3,  :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_score(tfidf_row):\n",
    "    \"\"\"\n",
    "    return the average of the non-zero values of the tf-idf vector representation of a sentence\n",
    "    \"\"\"\n",
    "    x = tfidf_row[tfidf_row != 0]\n",
    "    return x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07142857, 0.08333333, 0.125     , 0.14285714, 0.07692308,\n",
       "       0.09090909, 0.07142857, 0.07692308, 0.06666667, 0.07142857,\n",
       "       0.125     , 0.08333333, 0.1       , 0.06666667, 0.07142857,\n",
       "       0.04545455, 0.1       ])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.zeros(len(sentences))\n",
    "for i in range(len(sentences)):\n",
    "    score = get_sentence_score(X[i , :])\n",
    "\n",
    "    scores[i] = score\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07142857, 0.08333333, 0.125     , 0.14285714, 0.07692308,\n",
       "       0.09090909, 0.07142857, 0.07692308, 0.06666667, 0.07142857,\n",
       "       0.125     , 0.08333333, 0.1       , 0.06666667, 0.07142857,\n",
       "       0.04545455, 0.1       ])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.array(X.sum(axis=1)).flatten() / np.array((X != 0).sum(axis=1)).flatten()\n",
    "# np.array((X != 0).sum(axis=1)).flatten() = (X != 0).sum(axis=1).A1\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X.sum(axis=1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10,  2, 16, 12,  5,  1, 11,  4,  7,  6, 14,  0,  9, 13,  8, 15],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actually, we don't have to sort, since the order means, like the story line and the causality.\n",
    "sort_idx = np.argsort(-scores)\n",
    "sort_idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
